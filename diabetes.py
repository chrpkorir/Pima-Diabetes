# -*- coding: utf-8 -*-
"""Diabetes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JfxoDXFO_ic5eSWJZUQBgKJCODAtASge
"""

# Commented out IPython magic to ensure Python compatibility.
# import the libraries

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pickle
import os

# %matplotlib inline


from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.impute import SimpleImputer

# load the dataframe

df = pd.read_csv('diabetes.csv' )
df.head()

df.info()

# check if there are null values

df.isnull().values.any()

# Get correlation of each featuers in the dataset
corr = df.corr()
top_corr_features = corr.index
plt.figure(figsize=(20,20))

# plot the heatmap

g = sns.heatmap(df[top_corr_features].corr(), annot=True, cmap="RdYlGn")
df.corr()

# Check if the dataset is balanced

diabetes_true_count = len(df.loc[df["Outcome"]==True])
diabetes_false_count = len(df.loc[df["Outcome"]==False])

(diabetes_true_count, diabetes_false_count)

# Preparation of the dataset(splitting and normalization)

dfTrain = df[:650]
dfTest = df[650:750]
dfCheck = df[750:]

# split the dataset into feature and label variable
feature_cols = ['Pregnancies', 'Insulin', 'BloodPressure','BMI','Glucose','Age', 'SkinThickness', 'DiabetesPedigreeFunction']
predicted_class = ['Outcome']
X = df[feature_cols].values
y = df[predicted_class].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)

# separate label and features for both training and testing data set
# convert the data to numpy array

trainLabel = np.asarray(dfTrain['Outcome'])
trainData = np.asarray(dfTrain.drop('Outcome',1))
testLabel = np.asarray(dfTest['Outcome'])
testData = np.asarray(dfTest.drop('Outcome',1))

# normalize the data such that it has a mean of 0 and std of 1

means = np.mean(trainData, axis=0)  # check that new means equal 0
stds = np.std(trainData, axis=0)  # check that new stds equal 1

trainData = (trainData - means)/stds
testData = (testData - means)/stds

# Do imputation to the missing values

fill_values = SimpleImputer(missing_values=0, strategy="mean")
X_train = fill_values.fit_transform(X_train)
X_test = fill_values.fit_transform(X_test)

# Instantiate the model
logreg = LogisticRegression(max_iter=1000)

logreg.fit(X_train, y_train)

# predict the output for our model

y_pred = logreg.predict(X_test)
y_pred

X_test

# To improve accuracy import the metrics class to create confusion matrix

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
cnf_matrix

# plot the confusion matrix

class_names = [0,1] # names of classes
ax = plt.subplots
tick_marks = np.arange (len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)

# Create a heatmap

sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu", fmt='g')

plt.tight_layout()
plt.title("Confusion matrix")
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

# test accuracy

accuracy = metrics.accuracy_score(y_test, y_pred)
print("accuracy = ", accuracy * 100, "%")

# Save the model
if not os.path.exists('models'):
  os.makedirs('models')

MODEL_PATH = "models/logistic_reg.save"
pickle.dump(logreg, open(MODEL_PATH,'wb'))